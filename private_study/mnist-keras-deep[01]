{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWAJVC9P/7iDsJalcCD5O2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Supreme-YS/data_analysis/blob/master/mnist-keras-deep%5B01%5D\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_wu-Qa-S3VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c030f60-4d56-498e-fe98-8e01a1e3e836"
      },
      "source": [
        "    !cat /etc/issue.net"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ubuntu 18.04.5 LTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AshmNhvha2km",
        "outputId": "c9bed462-74b1-4737-f55d-b8688aeef60d"
      },
      "source": [
        "!head /proc/cpuinfo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYPHfSeNa6SD",
        "outputId": "9a2aa6ea-79da-4ae4-81e8-978470ee2a00"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMxfnoqsbZhm"
      },
      "source": [
        "파일 업로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpLK9EVTbbwG"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "7eKhyfKTbjiM",
        "outputId": "6f7ef9ad-8afb-4260-9ffa-c3aa33cb2999"
      },
      "source": [
        "myfile = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7fe28a0d-9b81-4e7f-ab24-e36fc7e296f3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7fe28a0d-9b81-4e7f-ab24-e36fc7e296f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving auto-mpg.csv to auto-mpg.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u41f6E0QbnVD",
        "outputId": "8ee4e390-466e-42a2-fd2c-4cbd81f800d5"
      },
      "source": [
        "myfile"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auto-mpg.csv': b'\\xef\\xbb\\xbf18.0,8,307.0,130.0,3504.,12.0,70,1,\"chevrolet chevelle malibu\"\\r\\n15.0,8,350.0,165.0,3693.,11.5,70,1,\"buick skylark 320\"\\r\\n18.0,8,318.0,150.0,3436.,11.0,70,1,\"plymouth satellite\"\\r\\n16.0,8,304.0,150.0,3433.,12.0,70,1,\"amc rebel sst\"\\r\\n17.0,8,302.0,140.0,3449.,10.5,70,1,\"ford torino\"\\r\\n15.0,8,429.0,198.0,4341.,10.0,70,1,\"ford galaxie 500\"\\r\\n14.0,8,454.0,220.0,4354., 9.0,70,1,\"chevrolet impala\"\\r\\n14.0,8,440.0,215.0,4312., 8.5,70,1,\"plymouth fury iii\"\\r\\n14.0,8,455.0,225.0,4425.,10.0,70,1,\"pontiac catalina\"\\r\\n15.0,8,390.0,190.0,3850., 8.5,70,1,\"amc ambassador dpl\"\\r\\n15.0,8,383.0,170.0,3563.,10.0,70,1,\"dodge challenger se\"\\r\\n14.0,8,340.0,160.0,3609., 8.0,70,1,\"plymouth \\'cuda 340\"\\r\\n15.0,8,400.0,150.0,3761., 9.5,70,1,\"chevrolet monte carlo\"\\r\\n14.0,8,455.0,225.0,3086.,10.0,70,1,\"buick estate wagon (sw)\"\\r\\n24.0,4,113.0,95.00,2372.,15.0,70,3,\"toyota corona mark ii\"\\r\\n22.0,6,198.0,95.00,2833.,15.5,70,1,\"plymouth duster\"\\r\\n18.0,6,199.0,97.00,2774.,15.5,70,1,\"amc hornet\"\\r\\n21.0,6,200.0,85.00,2587.,16.0,70,1,\"ford maverick\"\\r\\n27.0,4,97.00,88.00,2130.,14.5,70,3,\"datsun pl510\"\\r\\n26.0,4,97.00,46.00,1835.,20.5,70,2,\"volkswagen 1131 deluxe sedan\"\\r\\n25.0,4,110.0,87.00,2672.,17.5,70,2,\"peugeot 504\"\\r\\n24.0,4,107.0,90.00,2430.,14.5,70,2,\"audi 100 ls\"\\r\\n25.0,4,104.0,95.00,2375.,17.5,70,2,\"saab 99e\"\\r\\n26.0,4,121.0,113.0,2234.,12.5,70,2,\"bmw 2002\"\\r\\n21.0,6,199.0,90.00,2648.,15.0,70,1,\"amc gremlin\"\\r\\n10.0,8,360.0,215.0,4615.,14.0,70,1,\"ford f250\"\\r\\n10.0,8,307.0,200.0,4376.,15.0,70,1,\"chevy c20\"\\r\\n11.0,8,318.0,210.0,4382.,13.5,70,1,\"dodge d200\"\\r\\n9.0, 8,304.0,193.0,4732.,18.5,70,1,\"hi 1200d\"\\r\\n27.0,4,97.00,88.00,2130.,14.5,71,3,\"datsun pl510\"\\r\\n28.0,4,140.0,90.00,2264.,15.5,71,1,\"chevrolet vega 2300\"\\r\\n25.0,4,113.0,95.00,2228.,14.0,71,3,\"toyota corona\"\\r\\n25.0,4,98.00,?, 2046.,19.0,71,1,\"ford pinto\"\\r\\n19.0,6,232.0,100.0,2634.,13.0,71,1,\"amc gremlin\"\\r\\n16.0,6,225.0,105.0,3439.,15.5,71,1,\"plymouth satellite custom\"\\r\\n17.0,6,250.0,100.0,3329.,15.5,71,1,\"chevrolet chevelle malibu\"\\r\\n19.0,6,250.0,88.00,3302.,15.5,71,1,\"ford torino 500\"\\r\\n18.0,6,232.0,100.0,3288.,15.5,71,1,\"amc matador\"\\r\\n14.0,8,350.0,165.0,4209.,12.0,71,1,\"chevrolet impala\"\\r\\n14.0,8,400.0,175.0,4464.,11.5,71,1,\"pontiac catalina brougham\"\\r\\n14.0,8,351.0,153.0,4154.,13.5,71,1,\"ford galaxie 500\"\\r\\n14.0,8,318.0,150.0,4096.,13.0,71,1,\"plymouth fury iii\"\\r\\n12.0,8,383.0,180.0,4955.,11.5,71,1,\"dodge monaco (sw)\"\\r\\n13.0,8,400.0,170.0,4746.,12.0,71,1,\"ford country squire (sw)\"\\r\\n13.0,8,400.0,175.0,5140.,12.0,71,1,\"pontiac safari (sw)\"\\r\\n18.0,6,258.0,110.0,2962.,13.5,71,1,\"amc hornet sportabout (sw)\"\\r\\n22.0,4,140.0,72.00,2408.,19.0,71,1,\"chevrolet vega (sw)\"\\r\\n19.0,6,250.0,100.0,3282.,15.0,71,1,\"pontiac firebird\"\\r\\n18.0,6,250.0,88.00,3139.,14.5,71,1,\"ford mustang\"\\r\\n23.0,4,122.0,86.00,2220.,14.0,71,1,\"mercury capri 2000\"\\r\\n28.0,4,116.0,90.00,2123.,14.0,71,2,\"opel 1900\"\\r\\n30.0,4,79.00,70.00,2074.,19.5,71,2,\"peugeot 304\"\\r\\n30.0,4,88.00,76.00,2065.,14.5,71,2,\"fiat 124b\"\\r\\n31.0,4,71.00,65.00,1773.,19.0,71,3,\"toyota corolla 1200\"\\r\\n35.0,4,72.00,69.00,1613.,18.0,71,3,\"datsun 1200\"\\r\\n27.0,4,97.00,60.00,1834.,19.0,71,2,\"volkswagen model 111\"\\r\\n26.0,4,91.00,70.00,1955.,20.5,71,1,\"plymouth cricket\"\\r\\n24.0,4,113.0,95.00,2278.,15.5,72,3,\"toyota corona hardtop\"\\r\\n25.0,4,97.50,80.00,2126.,17.0,72,1,\"dodge colt hardtop\"\\r\\n23.0,4,97.00,54.00,2254.,23.5,72,2,\"volkswagen type 3\"\\r\\n20.0,4,140.0,90.00,2408.,19.5,72,1,\"chevrolet vega\"\\r\\n21.0,4,122.0,86.00,2226.,16.5,72,1,\"ford pinto runabout\"\\r\\n13.0,8,350.0,165.0,4274.,12.0,72,1,\"chevrolet impala\"\\r\\n14.0,8,400.0,175.0,4385.,12.0,72,1,\"pontiac catalina\"\\r\\n15.0,8,318.0,150.0,4135.,13.5,72,1,\"plymouth fury iii\"\\r\\n14.0,8,351.0,153.0,4129.,13.0,72,1,\"ford galaxie 500\"\\r\\n17.0,8,304.0,150.0,3672.,11.5,72,1,\"amc ambassador sst\"\\r\\n11.0,8,429.0,208.0,4633.,11.0,72,1,\"mercury marquis\"\\r\\n13.0,8,350.0,155.0,4502.,13.5,72,1,\"buick lesabre custom\"\\r\\n12.0,8,350.0,160.0,4456.,13.5,72,1,\"oldsmobile delta 88 royale\"\\r\\n13.0,8,400.0,190.0,4422.,12.5,72,1,\"chrysler newport royal\"\\r\\n19.0,3,70.00,97.00,2330.,13.5,72,3,\"mazda rx2 coupe\"\\r\\n15.0,8,304.0,150.0,3892.,12.5,72,1,\"amc matador (sw)\"\\r\\n13.0,8,307.0,130.0,4098.,14.0,72,1,\"chevrolet chevelle concours (sw)\"\\r\\n13.0,8,302.0,140.0,4294.,16.0,72,1,\"ford gran torino (sw)\"\\r\\n14.0,8,318.0,150.0,4077.,14.0,72,1,\"plymouth satellite custom (sw)\"\\r\\n18.0,4,121.0,112.0,2933.,14.5,72,2,\"volvo 145e (sw)\"\\r\\n22.0,4,121.0,76.00,2511.,18.0,72,2,\"volkswagen 411 (sw)\"\\r\\n21.0,4,120.0,87.00,2979.,19.5,72,2,\"peugeot 504 (sw)\"\\r\\n26.0,4,96.00,69.00,2189.,18.0,72,2,\"renault 12 (sw)\"\\r\\n22.0,4,122.0,86.00,2395.,16.0,72,1,\"ford pinto (sw)\"\\r\\n28.0,4,97.00,92.00,2288.,17.0,72,3,\"datsun 510 (sw)\"\\r\\n23.0,4,120.0,97.00,2506.,14.5,72,3,\"toyouta corona mark ii (sw)\"\\r\\n28.0,4,98.00,80.00,2164.,15.0,72,1,\"dodge colt (sw)\"\\r\\n27.0,4,97.00,88.00,2100.,16.5,72,3,\"toyota corolla 1600 (sw)\"\\r\\n13.0,8,350.0,175.0,4100.,13.0,73,1,\"buick century 350\"\\r\\n14.0,8,304.0,150.0,3672.,11.5,73,1,\"amc matador\"\\r\\n13.0,8,350.0,145.0,3988.,13.0,73,1,\"chevrolet malibu\"\\r\\n14.0,8,302.0,137.0,4042.,14.5,73,1,\"ford gran torino\"\\r\\n15.0,8,318.0,150.0,3777.,12.5,73,1,\"dodge coronet custom\"\\r\\n12.0,8,429.0,198.0,4952.,11.5,73,1,\"mercury marquis brougham\"\\r\\n13.0,8,400.0,150.0,4464.,12.0,73,1,\"chevrolet caprice classic\"\\r\\n13.0,8,351.0,158.0,4363.,13.0,73,1,\"ford ltd\"\\r\\n14.0,8,318.0,150.0,4237.,14.5,73,1,\"plymouth fury gran sedan\"\\r\\n13.0,8,440.0,215.0,4735.,11.0,73,1,\"chrysler new yorker brougham\"\\r\\n12.0,8,455.0,225.0,4951.,11.0,73,1,\"buick electra 225 custom\"\\r\\n13.0,8,360.0,175.0,3821.,11.0,73,1,\"amc ambassador brougham\"\\r\\n18.0,6,225.0,105.0,3121.,16.5,73,1,\"plymouth valiant\"\\r\\n16.0,6,250.0,100.0,3278.,18.0,73,1,\"chevrolet nova custom\"\\r\\n18.0,6,232.0,100.0,2945.,16.0,73,1,\"amc hornet\"\\r\\n18.0,6,250.0,88.00,3021.,16.5,73,1,\"ford maverick\"\\r\\n23.0,6,198.0,95.00,2904.,16.0,73,1,\"plymouth duster\"\\r\\n26.0,4,97.00,46.00,1950.,21.0,73,2,\"volkswagen super beetle\"\\r\\n11.0,8,400.0,150.0,4997.,14.0,73,1,\"chevrolet impala\"\\r\\n12.0,8,400.0,167.0,4906.,12.5,73,1,\"ford country\"\\r\\n13.0,8,360.0,170.0,4654.,13.0,73,1,\"plymouth custom suburb\"\\r\\n12.0,8,350.0,180.0,4499.,12.5,73,1,\"oldsmobile vista cruiser\"\\r\\n18.0,6,232.0,100.0,2789.,15.0,73,1,\"amc gremlin\"\\r\\n20.0,4,97.00,88.00,2279.,19.0,73,3,\"toyota carina\"\\r\\n21.0,4,140.0,72.00,2401.,19.5,73,1,\"chevrolet vega\"\\r\\n22.0,4,108.0,94.00,2379.,16.5,73,3,\"datsun 610\"\\r\\n18.0,3,70.00,90.00,2124.,13.5,73,3,\"maxda rx3\"\\r\\n19.0,4,122.0,85.00,2310.,18.5,73,1,\"ford pinto\"\\r\\n21.0,6,155.0,107.0,2472.,14.0,73,1,\"mercury capri v6\"\\r\\n26.0,4,98.00,90.00,2265.,15.5,73,2,\"fiat 124 sport coupe\"\\r\\n15.0,8,350.0,145.0,4082.,13.0,73,1,\"chevrolet monte carlo s\"\\r\\n16.0,8,400.0,230.0,4278.,9.50,73,1,\"pontiac grand prix\"\\r\\n29.0,4,68.00,49.00,1867.,19.5,73,2,\"fiat 128\"\\r\\n24.0,4,116.0,75.00,2158.,15.5,73,2,\"opel manta\"\\r\\n20.0,4,114.0,91.00,2582.,14.0,73,2,\"audi 100ls\"\\r\\n19.0,4,121.0,112.0,2868.,15.5,73,2,\"volvo 144ea\"\\r\\n15.0,8,318.0,150.0,3399.,11.0,73,1,\"dodge dart custom\"\\r\\n24.0,4,121.0,110.0,2660.,14.0,73,2,\"saab 99le\"\\r\\n20.0,6,156.0,122.0,2807.,13.5,73,3,\"toyota mark ii\"\\r\\n11.0,8,350.0,180.0,3664.,11.0,73,1,\"oldsmobile omega\"\\r\\n20.0,6,198.0,95.00,3102.,16.5,74,1,\"plymouth duster\"\\r\\n21.0,6,200.0,?, 2875.,17.0,74,1,\"ford maverick\"\\r\\n19.0,6,232.0,100.0,2901.,16.0,74,1,\"amc hornet\"\\r\\n15.0,6,250.0,100.0,3336.,17.0,74,1,\"chevrolet nova\"\\r\\n31.0,4,79.00,67.00,1950.,19.0,74,3,\"datsun b210\"\\r\\n26.0,4,122.0,80.00,2451.,16.5,74,1,\"ford pinto\"\\r\\n32.0,4,71.00,65.00,1836.,21.0,74,3,\"toyota corolla 1200\"\\r\\n25.0,4,140.0,75.00,2542.,17.0,74,1,\"chevrolet vega\"\\r\\n16.0,6,250.0,100.0,3781.,17.0,74,1,\"chevrolet chevelle malibu classic\"\\r\\n16.0,6,258.0,110.0,3632.,18.0,74,1,\"amc matador\"\\r\\n18.0,6,225.0,105.0,3613.,16.5,74,1,\"plymouth satellite sebring\"\\r\\n16.0,8,302.0,140.0,4141.,14.0,74,1,\"ford gran torino\"\\r\\n13.0,8,350.0,150.0,4699.,14.5,74,1,\"buick century luxus (sw)\"\\r\\n14.0,8,318.0,150.0,4457.,13.5,74,1,\"dodge coronet custom (sw)\"\\r\\n14.0,8,302.0,140.0,4638.,16.0,74,1,\"ford gran torino (sw)\"\\r\\n14.0,8,304.0,150.0,4257.,15.5,74,1,\"amc matador (sw)\"\\r\\n29.0,4,98.00,83.00,2219.,16.5,74,2,\"audi fox\"\\r\\n26.0,4,79.00,67.00,1963.,15.5,74,2,\"volkswagen dasher\"\\r\\n26.0,4,97.00,78.00,2300.,14.5,74,2,\"opel manta\"\\r\\n31.0,4,76.00,52.00,1649.,16.5,74,3,\"toyota corona\"\\r\\n32.0,4,83.00,61.00,2003.,19.0,74,3,\"datsun 710\"\\r\\n28.0,4,90.00,75.00,2125.,14.5,74,1,\"dodge colt\"\\r\\n24.0,4,90.00,75.00,2108.,15.5,74,2,\"fiat 128\"\\r\\n26.0,4,116.0,75.00,2246.,14.0,74,2,\"fiat 124 tc\"\\r\\n24.0,4,120.0,97.00,2489.,15.0,74,3,\"honda civic\"\\r\\n26.0,4,108.0,93.00,2391.,15.5,74,3,\"subaru\"\\r\\n31.0,4,79.00,67.00,2000.,16.0,74,2,\"fiat x1.9\"\\r\\n19.0,6,225.0,95.00,3264.,16.0,75,1,\"plymouth valiant custom\"\\r\\n18.0,6,250.0,105.0,3459.,16.0,75,1,\"chevrolet nova\"\\r\\n15.0,6,250.0,72.00,3432.,21.0,75,1,\"mercury monarch\"\\r\\n15.0,6,250.0,72.00,3158.,19.5,75,1,\"ford maverick\"\\r\\n16.0,8,400.0,170.0,4668.,11.5,75,1,\"pontiac catalina\"\\r\\n15.0,8,350.0,145.0,4440.,14.0,75,1,\"chevrolet bel air\"\\r\\n16.0,8,318.0,150.0,4498.,14.5,75,1,\"plymouth grand fury\"\\r\\n14.0,8,351.0,148.0,4657.,13.5,75,1,\"ford ltd\"\\r\\n17.0,6,231.0,110.0,3907.,21.0,75,1,\"buick century\"\\r\\n16.0,6,250.0,105.0,3897.,18.5,75,1,\"chevroelt chevelle malibu\"\\r\\n15.0,6,258.0,110.0,3730.,19.0,75,1,\"amc matador\"\\r\\n18.0,6,225.0,95.00,3785.,19.0,75,1,\"plymouth fury\"\\r\\n21.0,6,231.0,110.0,3039.,15.0,75,1,\"buick skyhawk\"\\r\\n20.0,8,262.0,110.0,3221.,13.5,75,1,\"chevrolet monza 2+2\"\\r\\n13.0,8,302.0,129.0,3169.,12.0,75,1,\"ford mustang ii\"\\r\\n29.0,4,97.00,75.00,2171.,16.0,75,3,\"toyota corolla\"\\r\\n23.0,4,140.0,83.00,2639.,17.0,75,1,\"ford pinto\"\\r\\n20.0,6,232.0,100.0,2914.,16.0,75,1,\"amc gremlin\"\\r\\n23.0,4,140.0,78.00,2592.,18.5,75,1,\"pontiac astro\"\\r\\n24.0,4,134.0,96.00,2702.,13.5,75,3,\"toyota corona\"\\r\\n25.0,4,90.00,71.00,2223.,16.5,75,2,\"volkswagen dasher\"\\r\\n24.0,4,119.0,97.00,2545.,17.0,75,3,\"datsun 710\"\\r\\n18.0,6,171.0,97.00,2984.,14.5,75,1,\"ford pinto\"\\r\\n29.0,4,90.00,70.00,1937.,14.0,75,2,\"volkswagen rabbit\"\\r\\n19.0,6,232.0,90.00,3211.,17.0,75,1,\"amc pacer\"\\r\\n23.0,4,115.0,95.00,2694.,15.0,75,2,\"audi 100ls\"\\r\\n23.0,4,120.0,88.00,2957.,17.0,75,2,\"peugeot 504\"\\r\\n22.0,4,121.0,98.00,2945.,14.5,75,2,\"volvo 244dl\"\\r\\n25.0,4,121.0,115.0,2671.,13.5,75,2,\"saab 99le\"\\r\\n33.0,4,91.00,53.00,1795.,17.5,75,3,\"honda civic cvcc\"\\r\\n28.0,4,107.0,86.00,2464.,15.5,76,2,\"fiat 131\"\\r\\n25.0,4,116.0,81.00,2220.,16.9,76,2,\"opel 1900\"\\r\\n25.0,4,140.0,92.00,2572.,14.9,76,1,\"capri ii\"\\r\\n26.0,4,98.00,79.00,2255.,17.7,76,1,\"dodge colt\"\\r\\n27.0,4,101.0,83.00,2202.,15.3,76,2,\"renault 12tl\"\\r\\n17.5,8,305.0,140.0,4215.,13.0,76,1,\"chevrolet chevelle malibu classic\"\\r\\n16.0,8,318.0,150.0,4190.,13.0,76,1,\"dodge coronet brougham\"\\r\\n15.5,8,304.0,120.0,3962.,13.9,76,1,\"amc matador\"\\r\\n14.5,8,351.0,152.0,4215.,12.8,76,1,\"ford gran torino\"\\r\\n22.0,6,225.0,100.0,3233.,15.4,76,1,\"plymouth valiant\"\\r\\n22.0,6,250.0,105.0,3353.,14.5,76,1,\"chevrolet nova\"\\r\\n24.0,6,200.0,81.00,3012.,17.6,76,1,\"ford maverick\"\\r\\n22.5,6,232.0,90.00,3085.,17.6,76,1,\"amc hornet\"\\r\\n29.0,4,85.00,52.00,2035.,22.2,76,1,\"chevrolet chevette\"\\r\\n24.5,4,98.00,60.00,2164.,22.1,76,1,\"chevrolet woody\"\\r\\n29.0,4,90.00,70.00,1937.,14.2,76,2,\"vw rabbit\"\\r\\n33.0,4,91.00,53.00,1795.,17.4,76,3,\"honda civic\"\\r\\n20.0,6,225.0,100.0,3651.,17.7,76,1,\"dodge aspen se\"\\r\\n18.0,6,250.0,78.00,3574.,21.0,76,1,\"ford granada ghia\"\\r\\n18.5,6,250.0,110.0,3645.,16.2,76,1,\"pontiac ventura sj\"\\r\\n17.5,6,258.0,95.00,3193.,17.8,76,1,\"amc pacer d/l\"\\r\\n29.5,4,97.00,71.00,1825.,12.2,76,2,\"volkswagen rabbit\"\\r\\n32.0,4,85.00,70.00,1990.,17.0,76,3,\"datsun b-210\"\\r\\n28.0,4,97.00,75.00,2155.,16.4,76,3,\"toyota corolla\"\\r\\n26.5,4,140.0,72.00,2565.,13.6,76,1,\"ford pinto\"\\r\\n20.0,4,130.0,102.0,3150.,15.7,76,2,\"volvo 245\"\\r\\n13.0,8,318.0,150.0,3940.,13.2,76,1,\"plymouth volare premier v8\"\\r\\n19.0,4,120.0,88.00,3270.,21.9,76,2,\"peugeot 504\"\\r\\n19.0,6,156.0,108.0,2930.,15.5,76,3,\"toyota mark ii\"\\r\\n16.5,6,168.0,120.0,3820.,16.7,76,2,\"mercedes-benz 280s\"\\r\\n16.5,8,350.0,180.0,4380.,12.1,76,1,\"cadillac seville\"\\r\\n13.0,8,350.0,145.0,4055.,12.0,76,1,\"chevy c10\"\\r\\n13.0,8,302.0,130.0,3870.,15.0,76,1,\"ford f108\"\\r\\n13.0,8,318.0,150.0,3755.,14.0,76,1,\"dodge d100\"\\r\\n31.5,4,98.00,68.00,2045.,18.5,77,3,\"honda accord cvcc\"\\r\\n30.0,4,111.0,80.00,2155.,14.8,77,1,\"buick opel isuzu deluxe\"\\r\\n36.0,4,79.00,58.00,1825.,18.6,77,2,\"renault 5 gtl\"\\r\\n25.5,4,122.0,96.00,2300.,15.5,77,1,\"plymouth arrow gs\"\\r\\n33.5,4,85.00,70.00,1945.,16.8,77,3,\"datsun f-10 hatchback\"\\r\\n17.5,8,305.0,145.0,3880.,12.5,77,1,\"chevrolet caprice classic\"\\r\\n17.0,8,260.0,110.0,4060.,19.0,77,1,\"oldsmobile cutlass supreme\"\\r\\n15.5,8,318.0,145.0,4140.,13.7,77,1,\"dodge monaco brougham\"\\r\\n15.0,8,302.0,130.0,4295.,14.9,77,1,\"mercury cougar brougham\"\\r\\n17.5,6,250.0,110.0,3520.,16.4,77,1,\"chevrolet concours\"\\r\\n20.5,6,231.0,105.0,3425.,16.9,77,1,\"buick skylark\"\\r\\n19.0,6,225.0,100.0,3630.,17.7,77,1,\"plymouth volare custom\"\\r\\n18.5,6,250.0,98.00,3525.,19.0,77,1,\"ford granada\"\\r\\n16.0,8,400.0,180.0,4220.,11.1,77,1,\"pontiac grand prix lj\"\\r\\n15.5,8,350.0,170.0,4165.,11.4,77,1,\"chevrolet monte carlo landau\"\\r\\n15.5,8,400.0,190.0,4325.,12.2,77,1,\"chrysler cordoba\"\\r\\n16.0,8,351.0,149.0,4335.,14.5,77,1,\"ford thunderbird\"\\r\\n29.0,4,97.00,78.00,1940.,14.5,77,2,\"volkswagen rabbit custom\"\\r\\n24.5,4,151.0,88.00,2740.,16.0,77,1,\"pontiac sunbird coupe\"\\r\\n26.0,4,97.00,75.00,2265.,18.2,77,3,\"toyota corolla liftback\"\\r\\n25.5,4,140.0,89.00,2755.,15.8,77,1,\"ford mustang ii 2+2\"\\r\\n30.5,4,98.00,63.00,2051.,17.0,77,1,\"chevrolet chevette\"\\r\\n33.5,4,98.00,83.00,2075.,15.9,77,1,\"dodge colt m/m\"\\r\\n30.0,4,97.00,67.00,1985.,16.4,77,3,\"subaru dl\"\\r\\n30.5,4,97.00,78.00,2190.,14.1,77,2,\"volkswagen dasher\"\\r\\n22.0,6,146.0,97.00,2815.,14.5,77,3,\"datsun 810\"\\r\\n21.5,4,121.0,110.0,2600.,12.8,77,2,\"bmw 320i\"\\r\\n21.5,3,80.00,110.0,2720.,13.5,77,3,\"mazda rx-4\"\\r\\n43.1,4,90.00,48.00,1985.,21.5,78,2,\"volkswagen rabbit custom diesel\"\\r\\n36.1,4,98.00,66.00,1800.,14.4,78,1,\"ford fiesta\"\\r\\n32.8,4,78.00,52.00,1985.,19.4,78,3,\"mazda glc deluxe\"\\r\\n39.4,4,85.00,70.00,2070.,18.6,78,3,\"datsun b210 gx\"\\r\\n36.1,4,91.00,60.00,1800.,16.4,78,3,\"honda civic cvcc\"\\r\\n19.9,8,260.0,110.0,3365.,15.5,78,1,\"oldsmobile cutlass salon brougham\"\\r\\n19.4,8,318.0,140.0,3735.,13.2,78,1,\"dodge diplomat\"\\r\\n20.2,8,302.0,139.0,3570.,12.8,78,1,\"mercury monarch ghia\"\\r\\n19.2,6,231.0,105.0,3535.,19.2,78,1,\"pontiac phoenix lj\"\\r\\n20.5,6,200.0,95.00,3155.,18.2,78,1,\"chevrolet malibu\"\\r\\n20.2,6,200.0,85.00,2965.,15.8,78,1,\"ford fairmont (auto)\"\\r\\n25.1,4,140.0,88.00,2720.,15.4,78,1,\"ford fairmont (man)\"\\r\\n20.5,6,225.0,100.0,3430.,17.2,78,1,\"plymouth volare\"\\r\\n19.4,6,232.0,90.00,3210.,17.2,78,1,\"amc concord\"\\r\\n20.6,6,231.0,105.0,3380.,15.8,78,1,\"buick century special\"\\r\\n20.8,6,200.0,85.00,3070.,16.7,78,1,\"mercury zephyr\"\\r\\n18.6,6,225.0,110.0,3620.,18.7,78,1,\"dodge aspen\"\\r\\n18.1,6,258.0,120.0,3410.,15.1,78,1,\"amc concord d/l\"\\r\\n19.2,8,305.0,145.0,3425.,13.2,78,1,\"chevrolet monte carlo landau\"\\r\\n17.7,6,231.0,165.0,3445.,13.4,78,1,\"buick regal sport coupe (turbo)\"\\r\\n18.1,8,302.0,139.0,3205.,11.2,78,1,\"ford futura\"\\r\\n17.5,8,318.0,140.0,4080.,13.7,78,1,\"dodge magnum xe\"\\r\\n30.0,4,98.00,68.00,2155.,16.5,78,1,\"chevrolet chevette\"\\r\\n27.5,4,134.0,95.00,2560.,14.2,78,3,\"toyota corona\"\\r\\n27.2,4,119.0,97.00,2300.,14.7,78,3,\"datsun 510\"\\r\\n30.9,4,105.0,75.00,2230.,14.5,78,1,\"dodge omni\"\\r\\n21.1,4,134.0,95.00,2515.,14.8,78,3,\"toyota celica gt liftback\"\\r\\n23.2,4,156.0,105.0,2745.,16.7,78,1,\"plymouth sapporo\"\\r\\n23.8,4,151.0,85.00,2855.,17.6,78,1,\"oldsmobile starfire sx\"\\r\\n23.9,4,119.0,97.00,2405.,14.9,78,3,\"datsun 200-sx\"\\r\\n20.3,5,131.0,103.0,2830.,15.9,78,2,\"audi 5000\"\\r\\n17.0,6,163.0,125.0,3140.,13.6,78,2,\"volvo 264gl\"\\r\\n21.6,4,121.0,115.0,2795.,15.7,78,2,\"saab 99gle\"\\r\\n16.2,6,163.0,133.0,3410.,15.8,78,2,\"peugeot 604sl\"\\r\\n31.5,4,89.00,71.00,1990.,14.9,78,2,\"volkswagen scirocco\"\\r\\n29.5,4,98.00,68.00,2135.,16.6,78,3,\"honda accord lx\"\\r\\n21.5,6,231.0,115.0,3245.,15.4,79,1,\"pontiac lemans v6\"\\r\\n19.8,6,200.0,85.00,2990.,18.2,79,1,\"mercury zephyr 6\"\\r\\n22.3,4,140.0,88.00,2890.,17.3,79,1,\"ford fairmont 4\"\\r\\n20.2,6,232.0,90.00,3265.,18.2,79,1,\"amc concord dl 6\"\\r\\n20.6,6,225.0,110.0,3360.,16.6,79,1,\"dodge aspen 6\"\\r\\n17.0,8,305.0,130.0,3840.,15.4,79,1,\"chevrolet caprice classic\"\\r\\n17.6,8,302.0,129.0,3725.,13.4,79,1,\"ford ltd landau\"\\r\\n16.5,8,351.0,138.0,3955.,13.2,79,1,\"mercury grand marquis\"\\r\\n18.2,8,318.0,135.0,3830.,15.2,79,1,\"dodge st. regis\"\\r\\n16.9,8,350.0,155.0,4360.,14.9,79,1,\"buick estate wagon (sw)\"\\r\\n15.5,8,351.0,142.0,4054.,14.3,79,1,\"ford country squire (sw)\"\\r\\n19.2,8,267.0,125.0,3605.,15.0,79,1,\"chevrolet malibu classic (sw)\"\\r\\n18.5,8,360.0,150.0,3940.,13.0,79,1,\"chrysler lebaron town @ country (sw)\"\\r\\n31.9,4,89.00,71.00,1925.,14.0,79,2,\"vw rabbit custom\"\\r\\n34.1,4,86.00,65.00,1975.,15.2,79,3,\"maxda glc deluxe\"\\r\\n35.7,4,98.00,80.00,1915.,14.4,79,1,\"dodge colt hatchback custom\"\\r\\n27.4,4,121.0,80.00,2670.,15.0,79,1,\"amc spirit dl\"\\r\\n25.4,5,183.0,77.00,3530.,20.1,79,2,\"mercedes benz 300d\"\\r\\n23.0,8,350.0,125.0,3900.,17.4,79,1,\"cadillac eldorado\"\\r\\n27.2,4,141.0,71.00,3190.,24.8,79,2,\"peugeot 504\"\\r\\n23.9,8,260.0,90.00,3420.,22.2,79,1,\"oldsmobile cutlass salon brougham\"\\r\\n34.2,4,105.0,70.00,2200.,13.2,79,1,\"plymouth horizon\"\\r\\n34.5,4,105.0,70.00,2150.,14.9,79,1,\"plymouth horizon tc3\"\\r\\n31.8,4,85.00,65.00,2020.,19.2,79,3,\"datsun 210\"\\r\\n37.3,4,91.00,69.00,2130.,14.7,79,2,\"fiat strada custom\"\\r\\n28.4,4,151.0,90.00,2670.,16.0,79,1,\"buick skylark limited\"\\r\\n28.8,6,173.0,115.0,2595.,11.3,79,1,\"chevrolet citation\"\\r\\n26.8,6,173.0,115.0,2700.,12.9,79,1,\"oldsmobile omega brougham\"\\r\\n33.5,4,151.0,90.00,2556.,13.2,79,1,\"pontiac phoenix\"\\r\\n41.5,4,98.00,76.00,2144.,14.7,80,2,\"vw rabbit\"\\r\\n38.1,4,89.00,60.00,1968.,18.8,80,3,\"toyota corolla tercel\"\\r\\n32.1,4,98.00,70.00,2120.,15.5,80,1,\"chevrolet chevette\"\\r\\n37.2,4,86.00,65.00,2019.,16.4,80,3,\"datsun 310\"\\r\\n28.0,4,151.0,90.00,2678.,16.5,80,1,\"chevrolet citation\"\\r\\n26.4,4,140.0,88.00,2870.,18.1,80,1,\"ford fairmont\"\\r\\n24.3,4,151.0,90.00,3003.,20.1,80,1,\"amc concord\"\\r\\n19.1,6,225.0,90.00,3381.,18.7,80,1,\"dodge aspen\"\\r\\n34.3,4,97.00,78.00,2188.,15.8,80,2,\"audi 4000\"\\r\\n29.8,4,134.0,90.00,2711.,15.5,80,3,\"toyota corona liftback\"\\r\\n31.3,4,120.0,75.00,2542.,17.5,80,3,\"mazda 626\"\\r\\n37.0,4,119.0,92.00,2434.,15.0,80,3,\"datsun 510 hatchback\"\\r\\n32.2,4,108.0,75.00,2265.,15.2,80,3,\"toyota corolla\"\\r\\n46.6,4,86.00,65.00,2110.,17.9,80,3,\"mazda glc\"\\r\\n27.9,4,156.0,105.0,2800.,14.4,80,1,\"dodge colt\"\\r\\n40.8,4,85.00,65.00,2110.,19.2,80,3,\"datsun 210\"\\r\\n44.3,4,90.00,48.00,2085.,21.7,80,2,\"vw rabbit c (diesel)\"\\r\\n43.4,4,90.00,48.00,2335.,23.7,80,2,\"vw dasher (diesel)\"\\r\\n36.4,5,121.0,67.00,2950.,19.9,80,2,\"audi 5000s (diesel)\"\\r\\n30.0,4,146.0,67.00,3250.,21.8,80,2,\"mercedes-benz 240d\"\\r\\n44.6,4,91.00,67.00,1850.,13.8,80,3,\"honda civic 1500 gl\"\\r\\n40.9,4,85.00,?, 1835.,17.3,80,2,\"renault lecar deluxe\"\\r\\n33.8,4,97.00,67.00,2145.,18.0,80,3,\"subaru dl\"\\r\\n29.8,4,89.00,62.00,1845.,15.3,80,2,\"vokswagen rabbit\"\\r\\n32.7,6,168.0,132.0,2910.,11.4,80,3,\"datsun 280-zx\"\\r\\n23.7,3,70.00,100.0,2420.,12.5,80,3,\"mazda rx-7 gs\"\\r\\n35.0,4,122.0,88.00,2500.,15.1,80,2,\"triumph tr7 coupe\"\\r\\n23.6,4,140.0,?, 2905.,14.3,80,1,\"ford mustang cobra\"\\r\\n32.4,4,107.0,72.00,2290.,17.0,80,3,\"honda accord\"\\r\\n27.2,4,135.0,84.00,2490.,15.7,81,1,\"plymouth reliant\"\\r\\n26.6,4,151.0,84.00,2635.,16.4,81,1,\"buick skylark\"\\r\\n25.8,4,156.0,92.00,2620.,14.4,81,1,\"dodge aries wagon (sw)\"\\r\\n23.5,6,173.0,110.0,2725.,12.6,81,1,\"chevrolet citation\"\\r\\n30.0,4,135.0,84.00,2385.,12.9,81,1,\"plymouth reliant\"\\r\\n39.1,4,79.00,58.00,1755.,16.9,81,3,\"toyota starlet\"\\r\\n39.0,4,86.00,64.00,1875.,16.4,81,1,\"plymouth champ\"\\r\\n35.1,4,81.00,60.00,1760.,16.1,81,3,\"honda civic 1300\"\\r\\n32.3,4,97.00,67.00,2065.,17.8,81,3,\"subaru\"\\r\\n37.0,4,85.00,65.00,1975.,19.4,81,3,\"datsun 210 mpg\"\\r\\n37.7,4,89.00,62.00,2050.,17.3,81,3,\"toyota tercel\"\\r\\n34.1,4,91.00,68.00,1985.,16.0,81,3,\"mazda glc 4\"\\r\\n34.7,4,105.0,63.00,2215.,14.9,81,1,\"plymouth horizon 4\"\\r\\n34.4,4,98.00,65.00,2045.,16.2,81,1,\"ford escort 4w\"\\r\\n29.9,4,98.00,65.00,2380.,20.7,81,1,\"ford escort 2h\"\\r\\n33.0,4,105.0,74.00,2190.,14.2,81,2,\"volkswagen jetta\"\\r\\n34.5,4,100.0,?, 2320.,15.8,81,2,\"renault 18i\"\\r\\n33.7,4,107.0,75.00,2210.,14.4,81,3,\"honda prelude\"\\r\\n32.4,4,108.0,75.00,2350.,16.8,81,3,\"toyota corolla\"\\r\\n32.9,4,119.0,100.0,2615.,14.8,81,3,\"datsun 200sx\"\\r\\n31.6,4,120.0,74.00,2635.,18.3,81,3,\"mazda 626\"\\r\\n28.1,4,141.0,80.00,3230.,20.4,81,2,\"peugeot 505s turbo diesel\"\\r\\n30.7,6,145.0,76.00,3160.,19.6,81,2,\"volvo diesel\"\\r\\n25.4,6,168.0,116.0,2900.,12.6,81,3,\"toyota cressida\"\\r\\n24.2,6,146.0,120.0,2930.,13.8,81,3,\"datsun 810 maxima\"\\r\\n22.4,6,231.0,110.0,3415.,15.8,81,1,\"buick century\"\\r\\n26.6,8,350.0,105.0,3725.,19.0,81,1,\"oldsmobile cutlass ls\"\\r\\n20.2,6,200.0,88.00,3060.,17.1,81,1,\"ford granada gl\"\\r\\n17.6,6,225.0,85.00,3465.,16.6,81,1,\"chrysler lebaron salon\"\\r\\n28.0,4,112.0,88.00,2605.,19.6,82,1,\"chevrolet cavalier\"\\r\\n27.0,4,112.0,88.00,2640.,18.6,82,1,\"chevrolet cavalier wagon\"\\r\\n34.0,4,112.0,88.00,2395.,18.0,82,1,\"chevrolet cavalier 2-door\"\\r\\n31.0,4,112.0,85.00,2575.,16.2,82,1,\"pontiac j2000 se hatchback\"\\r\\n29.0,4,135.0,84.00,2525.,16.0,82,1,\"dodge aries se\"\\r\\n27.0,4,151.0,90.00,2735.,18.0,82,1,\"pontiac phoenix\"\\r\\n24.0,4,140.0,92.00,2865.,16.4,82,1,\"ford fairmont futura\"\\r\\n23.0,4,151.0,?, 3035.,20.5,82,1,\"amc concord dl\"\\r\\n36.0,4,105.0,74.00,1980.,15.3,82,2,\"volkswagen rabbit l\"\\r\\n37.0,4,91.00,68.00,2025.,18.2,82,3,\"mazda glc custom l\"\\r\\n31.0,4,91.00,68.00,1970.,17.6,82,3,\"mazda glc custom\"\\r\\n38.0,4,105.0,63.00,2125.,14.7,82,1,\"plymouth horizon miser\"\\r\\n36.0,4,98.00,70.00,2125.,17.3,82,1,\"mercury lynx l\"\\r\\n36.0,4,120.0,88.00,2160.,14.5,82,3,\"nissan stanza xe\"\\r\\n36.0,4,107.0,75.00,2205.,14.5,82,3,\"honda accord\"\\r\\n34.0,4,108.0,70.00,2245, 16.9,82,3,\"toyota corolla\"\\r\\n38.0,4,91.00,67.00,1965.,15.0,82,3,\"honda civic\"\\r\\n32.0,4,91.00,67.00,1965.,15.7,82,3,\"honda civic (auto)\"\\r\\n38.0,4,91.00,67.00,1995.,16.2,82,3,\"datsun 310 gx\"\\r\\n25.0,6,181.0,110.0,2945.,16.4,82,1,\"buick century limited\"\\r\\n38.0,6,262.0,85.00,3015.,17.0,82,1,\"oldsmobile cutlass ciera (diesel)\"\\r\\n26.0,4,156.0,92.00,2585.,14.5,82,1,\"chrysler lebaron medallion\"\\r\\n22.0,6,232.0,112.0,2835, 14.7,82,1,\"ford granada l\"\\r\\n32.0,4,144.0,96.00,2665.,13.9,82,3,\"toyota celica gt\"\\r\\n36.0,4,135.0,84.00,2370.,13.0,82,1,\"dodge charger 2.2\"\\r\\n27.0,4,151.0,90.00,2950.,17.3,82,1,\"chevrolet camaro\"\\r\\n27.0,4,140.0,86.00,2790.,15.6,82,1,\"ford mustang gl\"\\r\\n44.0,4,97.00,52.00,2130.,24.6,82,2,\"vw pickup\"\\r\\n32.0,4,135.0,84.00,2295.,11.6,82,1,\"dodge rampage\"\\r\\n28.0,4,120.0,79.00,2625.,18.6,82,1,\"ford ranger\"\\r\\n31.0,4,119.0,82.00,2720.,19.4,82,1,\"chevy s-10\"\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8aKNfBu7bxRC",
        "outputId": "d1253933-332e-4982-d40a-06849ab9b1a0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasets = pd.read_csv('auto-mpg.csv', encoding='utf-8')\n",
        "datasets.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>18.0</th>\n",
              "      <th>8</th>\n",
              "      <th>307.0</th>\n",
              "      <th>130.0</th>\n",
              "      <th>3504.</th>\n",
              "      <th>12.0</th>\n",
              "      <th>70</th>\n",
              "      <th>1</th>\n",
              "      <th>chevrolet chevelle malibu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>buick skylark 320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>plymouth satellite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>amc rebel sst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford torino</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>429.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>4341.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford galaxie 500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   18.0  8  307.0  130.0   3504.  12.0  70  1 chevrolet chevelle malibu\n",
              "0  15.0  8  350.0  165.0  3693.0  11.5  70  1         buick skylark 320\n",
              "1  18.0  8  318.0  150.0  3436.0  11.0  70  1        plymouth satellite\n",
              "2  16.0  8  304.0  150.0  3433.0  12.0  70  1             amc rebel sst\n",
              "3  17.0  8  302.0  140.0  3449.0  10.5  70  1               ford torino\n",
              "4  15.0  8  429.0  198.0  4341.0  10.0  70  1          ford galaxie 500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krxCakqLcCvM"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOxsE6elcdoj"
      },
      "source": [
        "(x_train, y_train) , (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISDMZDtcelb"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvzs8Mfcj_i"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "input = keras.layers.Input(shape=(784,), name='input')\n",
        "\n",
        "# model.add( keras.layers.Dense(256 , activation='sigmoid', input_shape=(784, ), name='input') ) # input layer\n",
        "\n",
        "hidden1 = Dense(256, activation='sigmoid', name='hidden1')(input)\n",
        "hidden2 = Dense(128, activation='sigmoid', name='hidden2')(hidden1)\n",
        "hidden3 = Dense(64, activation='sigmoid', name='hidden3')(hidden2)\n",
        "hidden4 = Dense(32, activation='sigmoid', name='hidden4')(hidden3)\n",
        "\n",
        "output  = Dense(10, activation='softmax', name='output')(hidden4)\n",
        "\n",
        "model = Model(inputs=[input] , outputs=[output])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcoFuX2ocquj"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "model.compile(optimizer = optimizer,\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'] ) "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2souJrcVeSBn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                  test_size = 0.3,\n",
        "                                                  random_state = 100)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6_fKcJqctet",
        "outputId": "ff5abeac-4677-4ac8-e0f5-b7dc08f07b8d"
      },
      "source": [
        "callbakcs = [EarlyStopping(monitor='val_accuracy', patience=3)]\n",
        "history = model.fit(x_train, y_train, batch_size = 2000, epochs=300, validation_data=(x_val, y_val))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.7034 - accuracy: 0.8344 - val_loss: 0.7558 - val_accuracy: 0.8098\n",
            "Epoch 2/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.7018 - accuracy: 0.8352 - val_loss: 0.7544 - val_accuracy: 0.8099\n",
            "Epoch 3/300\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 0.7003 - accuracy: 0.8355 - val_loss: 0.7531 - val_accuracy: 0.8106\n",
            "Epoch 4/300\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 0.6988 - accuracy: 0.8355 - val_loss: 0.7518 - val_accuracy: 0.8109\n",
            "Epoch 5/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6973 - accuracy: 0.8362 - val_loss: 0.7504 - val_accuracy: 0.8116\n",
            "Epoch 6/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6958 - accuracy: 0.8367 - val_loss: 0.7492 - val_accuracy: 0.8116\n",
            "Epoch 7/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6943 - accuracy: 0.8370 - val_loss: 0.7479 - val_accuracy: 0.8125\n",
            "Epoch 8/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6928 - accuracy: 0.8373 - val_loss: 0.7466 - val_accuracy: 0.8124\n",
            "Epoch 9/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6913 - accuracy: 0.8378 - val_loss: 0.7454 - val_accuracy: 0.8127\n",
            "Epoch 10/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6898 - accuracy: 0.8380 - val_loss: 0.7441 - val_accuracy: 0.8128\n",
            "Epoch 11/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6884 - accuracy: 0.8384 - val_loss: 0.7428 - val_accuracy: 0.8132\n",
            "Epoch 12/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6869 - accuracy: 0.8385 - val_loss: 0.7415 - val_accuracy: 0.8132\n",
            "Epoch 13/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6855 - accuracy: 0.8389 - val_loss: 0.7403 - val_accuracy: 0.8142\n",
            "Epoch 14/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6840 - accuracy: 0.8395 - val_loss: 0.7390 - val_accuracy: 0.8149\n",
            "Epoch 15/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6826 - accuracy: 0.8400 - val_loss: 0.7377 - val_accuracy: 0.8146\n",
            "Epoch 16/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6811 - accuracy: 0.8401 - val_loss: 0.7365 - val_accuracy: 0.8152\n",
            "Epoch 17/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6797 - accuracy: 0.8407 - val_loss: 0.7353 - val_accuracy: 0.8159\n",
            "Epoch 18/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6782 - accuracy: 0.8408 - val_loss: 0.7340 - val_accuracy: 0.8164\n",
            "Epoch 19/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6768 - accuracy: 0.8414 - val_loss: 0.7329 - val_accuracy: 0.8167\n",
            "Epoch 20/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6754 - accuracy: 0.8415 - val_loss: 0.7315 - val_accuracy: 0.8173\n",
            "Epoch 21/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6740 - accuracy: 0.8424 - val_loss: 0.7303 - val_accuracy: 0.8171\n",
            "Epoch 22/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6726 - accuracy: 0.8422 - val_loss: 0.7291 - val_accuracy: 0.8182\n",
            "Epoch 23/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6712 - accuracy: 0.8427 - val_loss: 0.7278 - val_accuracy: 0.8182\n",
            "Epoch 24/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6698 - accuracy: 0.8431 - val_loss: 0.7267 - val_accuracy: 0.8181\n",
            "Epoch 25/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6684 - accuracy: 0.8432 - val_loss: 0.7254 - val_accuracy: 0.8187\n",
            "Epoch 26/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6671 - accuracy: 0.8435 - val_loss: 0.7243 - val_accuracy: 0.8189\n",
            "Epoch 27/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6657 - accuracy: 0.8442 - val_loss: 0.7231 - val_accuracy: 0.8193\n",
            "Epoch 28/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6643 - accuracy: 0.8444 - val_loss: 0.7220 - val_accuracy: 0.8194\n",
            "Epoch 29/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6629 - accuracy: 0.8450 - val_loss: 0.7207 - val_accuracy: 0.8196\n",
            "Epoch 30/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6616 - accuracy: 0.8450 - val_loss: 0.7194 - val_accuracy: 0.8202\n",
            "Epoch 31/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6602 - accuracy: 0.8456 - val_loss: 0.7183 - val_accuracy: 0.8205\n",
            "Epoch 32/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6589 - accuracy: 0.8461 - val_loss: 0.7171 - val_accuracy: 0.8209\n",
            "Epoch 33/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.6576 - accuracy: 0.8463 - val_loss: 0.7160 - val_accuracy: 0.8215\n",
            "Epoch 34/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6561 - accuracy: 0.8469 - val_loss: 0.7150 - val_accuracy: 0.8216\n",
            "Epoch 35/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6548 - accuracy: 0.8470 - val_loss: 0.7137 - val_accuracy: 0.8216\n",
            "Epoch 36/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6535 - accuracy: 0.8475 - val_loss: 0.7125 - val_accuracy: 0.8214\n",
            "Epoch 37/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6522 - accuracy: 0.8477 - val_loss: 0.7116 - val_accuracy: 0.8221\n",
            "Epoch 38/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6508 - accuracy: 0.8483 - val_loss: 0.7104 - val_accuracy: 0.8222\n",
            "Epoch 39/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6495 - accuracy: 0.8485 - val_loss: 0.7092 - val_accuracy: 0.8228\n",
            "Epoch 40/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6481 - accuracy: 0.8487 - val_loss: 0.7080 - val_accuracy: 0.8222\n",
            "Epoch 41/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6468 - accuracy: 0.8490 - val_loss: 0.7070 - val_accuracy: 0.8226\n",
            "Epoch 42/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6456 - accuracy: 0.8494 - val_loss: 0.7058 - val_accuracy: 0.8228\n",
            "Epoch 43/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6443 - accuracy: 0.8497 - val_loss: 0.7047 - val_accuracy: 0.8237\n",
            "Epoch 44/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6429 - accuracy: 0.8504 - val_loss: 0.7035 - val_accuracy: 0.8237\n",
            "Epoch 45/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6417 - accuracy: 0.8502 - val_loss: 0.7023 - val_accuracy: 0.8238\n",
            "Epoch 46/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.6404 - accuracy: 0.8505 - val_loss: 0.7013 - val_accuracy: 0.8242\n",
            "Epoch 47/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6391 - accuracy: 0.8510 - val_loss: 0.7002 - val_accuracy: 0.8246\n",
            "Epoch 48/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6378 - accuracy: 0.8516 - val_loss: 0.6990 - val_accuracy: 0.8253\n",
            "Epoch 49/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6365 - accuracy: 0.8516 - val_loss: 0.6980 - val_accuracy: 0.8249\n",
            "Epoch 50/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.6352 - accuracy: 0.8525 - val_loss: 0.6970 - val_accuracy: 0.8257\n",
            "Epoch 51/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6340 - accuracy: 0.8525 - val_loss: 0.6959 - val_accuracy: 0.8254\n",
            "Epoch 52/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6327 - accuracy: 0.8530 - val_loss: 0.6949 - val_accuracy: 0.8258\n",
            "Epoch 53/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6315 - accuracy: 0.8530 - val_loss: 0.6936 - val_accuracy: 0.8266\n",
            "Epoch 54/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6302 - accuracy: 0.8537 - val_loss: 0.6928 - val_accuracy: 0.8262\n",
            "Epoch 55/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.6290 - accuracy: 0.8539 - val_loss: 0.6916 - val_accuracy: 0.8263\n",
            "Epoch 56/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6277 - accuracy: 0.8544 - val_loss: 0.6907 - val_accuracy: 0.8265\n",
            "Epoch 57/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6265 - accuracy: 0.8545 - val_loss: 0.6895 - val_accuracy: 0.8269\n",
            "Epoch 58/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6253 - accuracy: 0.8546 - val_loss: 0.6884 - val_accuracy: 0.8268\n",
            "Epoch 59/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6241 - accuracy: 0.8551 - val_loss: 0.6874 - val_accuracy: 0.8277\n",
            "Epoch 60/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6228 - accuracy: 0.8558 - val_loss: 0.6865 - val_accuracy: 0.8271\n",
            "Epoch 61/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6216 - accuracy: 0.8558 - val_loss: 0.6854 - val_accuracy: 0.8274\n",
            "Epoch 62/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6204 - accuracy: 0.8562 - val_loss: 0.6843 - val_accuracy: 0.8280\n",
            "Epoch 63/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6192 - accuracy: 0.8563 - val_loss: 0.6832 - val_accuracy: 0.8283\n",
            "Epoch 64/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6180 - accuracy: 0.8568 - val_loss: 0.6823 - val_accuracy: 0.8283\n",
            "Epoch 65/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6168 - accuracy: 0.8569 - val_loss: 0.6812 - val_accuracy: 0.8284\n",
            "Epoch 66/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6156 - accuracy: 0.8571 - val_loss: 0.6801 - val_accuracy: 0.8286\n",
            "Epoch 67/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6144 - accuracy: 0.8578 - val_loss: 0.6792 - val_accuracy: 0.8292\n",
            "Epoch 68/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.6132 - accuracy: 0.8578 - val_loss: 0.6781 - val_accuracy: 0.8291\n",
            "Epoch 69/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6120 - accuracy: 0.8581 - val_loss: 0.6772 - val_accuracy: 0.8289\n",
            "Epoch 70/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.6108 - accuracy: 0.8581 - val_loss: 0.6761 - val_accuracy: 0.8291\n",
            "Epoch 71/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.6096 - accuracy: 0.8585 - val_loss: 0.6750 - val_accuracy: 0.8298\n",
            "Epoch 72/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6084 - accuracy: 0.8590 - val_loss: 0.6742 - val_accuracy: 0.8298\n",
            "Epoch 73/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6073 - accuracy: 0.8594 - val_loss: 0.6730 - val_accuracy: 0.8298\n",
            "Epoch 74/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6061 - accuracy: 0.8594 - val_loss: 0.6721 - val_accuracy: 0.8309\n",
            "Epoch 75/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6049 - accuracy: 0.8598 - val_loss: 0.6710 - val_accuracy: 0.8309\n",
            "Epoch 76/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.6038 - accuracy: 0.8601 - val_loss: 0.6703 - val_accuracy: 0.8309\n",
            "Epoch 77/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6026 - accuracy: 0.8602 - val_loss: 0.6692 - val_accuracy: 0.8316\n",
            "Epoch 78/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6015 - accuracy: 0.8604 - val_loss: 0.6681 - val_accuracy: 0.8311\n",
            "Epoch 79/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.6003 - accuracy: 0.8607 - val_loss: 0.6673 - val_accuracy: 0.8313\n",
            "Epoch 80/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5992 - accuracy: 0.8611 - val_loss: 0.6663 - val_accuracy: 0.8317\n",
            "Epoch 81/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5981 - accuracy: 0.8613 - val_loss: 0.6653 - val_accuracy: 0.8324\n",
            "Epoch 82/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5969 - accuracy: 0.8612 - val_loss: 0.6643 - val_accuracy: 0.8327\n",
            "Epoch 83/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5958 - accuracy: 0.8619 - val_loss: 0.6635 - val_accuracy: 0.8324\n",
            "Epoch 84/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5947 - accuracy: 0.8621 - val_loss: 0.6625 - val_accuracy: 0.8332\n",
            "Epoch 85/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5936 - accuracy: 0.8622 - val_loss: 0.6618 - val_accuracy: 0.8329\n",
            "Epoch 86/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5925 - accuracy: 0.8626 - val_loss: 0.6606 - val_accuracy: 0.8334\n",
            "Epoch 87/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5914 - accuracy: 0.8627 - val_loss: 0.6597 - val_accuracy: 0.8332\n",
            "Epoch 88/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5902 - accuracy: 0.8630 - val_loss: 0.6588 - val_accuracy: 0.8335\n",
            "Epoch 89/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5891 - accuracy: 0.8631 - val_loss: 0.6579 - val_accuracy: 0.8337\n",
            "Epoch 90/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5880 - accuracy: 0.8632 - val_loss: 0.6570 - val_accuracy: 0.8336\n",
            "Epoch 91/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5869 - accuracy: 0.8636 - val_loss: 0.6559 - val_accuracy: 0.8343\n",
            "Epoch 92/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5858 - accuracy: 0.8642 - val_loss: 0.6551 - val_accuracy: 0.8340\n",
            "Epoch 93/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5848 - accuracy: 0.8644 - val_loss: 0.6543 - val_accuracy: 0.8345\n",
            "Epoch 94/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5837 - accuracy: 0.8643 - val_loss: 0.6534 - val_accuracy: 0.8346\n",
            "Epoch 95/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5826 - accuracy: 0.8649 - val_loss: 0.6524 - val_accuracy: 0.8348\n",
            "Epoch 96/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5815 - accuracy: 0.8651 - val_loss: 0.6515 - val_accuracy: 0.8351\n",
            "Epoch 97/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5804 - accuracy: 0.8652 - val_loss: 0.6508 - val_accuracy: 0.8349\n",
            "Epoch 98/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5794 - accuracy: 0.8656 - val_loss: 0.6498 - val_accuracy: 0.8351\n",
            "Epoch 99/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5783 - accuracy: 0.8661 - val_loss: 0.6490 - val_accuracy: 0.8348\n",
            "Epoch 100/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5773 - accuracy: 0.8660 - val_loss: 0.6479 - val_accuracy: 0.8357\n",
            "Epoch 101/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5761 - accuracy: 0.8665 - val_loss: 0.6471 - val_accuracy: 0.8354\n",
            "Epoch 102/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5751 - accuracy: 0.8667 - val_loss: 0.6462 - val_accuracy: 0.8364\n",
            "Epoch 103/300\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.5740 - accuracy: 0.8667 - val_loss: 0.6453 - val_accuracy: 0.8361\n",
            "Epoch 104/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5730 - accuracy: 0.8672 - val_loss: 0.6446 - val_accuracy: 0.8362\n",
            "Epoch 105/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5720 - accuracy: 0.8675 - val_loss: 0.6436 - val_accuracy: 0.8369\n",
            "Epoch 106/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5709 - accuracy: 0.8676 - val_loss: 0.6427 - val_accuracy: 0.8369\n",
            "Epoch 107/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5699 - accuracy: 0.8680 - val_loss: 0.6420 - val_accuracy: 0.8371\n",
            "Epoch 108/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5689 - accuracy: 0.8680 - val_loss: 0.6412 - val_accuracy: 0.8369\n",
            "Epoch 109/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5678 - accuracy: 0.8685 - val_loss: 0.6402 - val_accuracy: 0.8372\n",
            "Epoch 110/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5668 - accuracy: 0.8686 - val_loss: 0.6392 - val_accuracy: 0.8375\n",
            "Epoch 111/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5657 - accuracy: 0.8692 - val_loss: 0.6387 - val_accuracy: 0.8377\n",
            "Epoch 112/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5647 - accuracy: 0.8693 - val_loss: 0.6379 - val_accuracy: 0.8380\n",
            "Epoch 113/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5637 - accuracy: 0.8694 - val_loss: 0.6370 - val_accuracy: 0.8381\n",
            "Epoch 114/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5626 - accuracy: 0.8697 - val_loss: 0.6360 - val_accuracy: 0.8384\n",
            "Epoch 115/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5616 - accuracy: 0.8701 - val_loss: 0.6352 - val_accuracy: 0.8384\n",
            "Epoch 116/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5606 - accuracy: 0.8706 - val_loss: 0.6343 - val_accuracy: 0.8387\n",
            "Epoch 117/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5596 - accuracy: 0.8708 - val_loss: 0.6336 - val_accuracy: 0.8384\n",
            "Epoch 118/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5586 - accuracy: 0.8709 - val_loss: 0.6327 - val_accuracy: 0.8389\n",
            "Epoch 119/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5576 - accuracy: 0.8714 - val_loss: 0.6320 - val_accuracy: 0.8394\n",
            "Epoch 120/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5566 - accuracy: 0.8718 - val_loss: 0.6311 - val_accuracy: 0.8393\n",
            "Epoch 121/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5556 - accuracy: 0.8720 - val_loss: 0.6303 - val_accuracy: 0.8396\n",
            "Epoch 122/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5546 - accuracy: 0.8719 - val_loss: 0.6295 - val_accuracy: 0.8397\n",
            "Epoch 123/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5536 - accuracy: 0.8724 - val_loss: 0.6288 - val_accuracy: 0.8402\n",
            "Epoch 124/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5526 - accuracy: 0.8726 - val_loss: 0.6280 - val_accuracy: 0.8402\n",
            "Epoch 125/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5517 - accuracy: 0.8725 - val_loss: 0.6272 - val_accuracy: 0.8400\n",
            "Epoch 126/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5506 - accuracy: 0.8730 - val_loss: 0.6265 - val_accuracy: 0.8408\n",
            "Epoch 127/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5497 - accuracy: 0.8734 - val_loss: 0.6256 - val_accuracy: 0.8408\n",
            "Epoch 128/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5488 - accuracy: 0.8735 - val_loss: 0.6248 - val_accuracy: 0.8407\n",
            "Epoch 129/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5478 - accuracy: 0.8736 - val_loss: 0.6241 - val_accuracy: 0.8416\n",
            "Epoch 130/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5469 - accuracy: 0.8740 - val_loss: 0.6233 - val_accuracy: 0.8414\n",
            "Epoch 131/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5459 - accuracy: 0.8742 - val_loss: 0.6225 - val_accuracy: 0.8422\n",
            "Epoch 132/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5449 - accuracy: 0.8746 - val_loss: 0.6217 - val_accuracy: 0.8421\n",
            "Epoch 133/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5440 - accuracy: 0.8747 - val_loss: 0.6210 - val_accuracy: 0.8415\n",
            "Epoch 134/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5431 - accuracy: 0.8745 - val_loss: 0.6203 - val_accuracy: 0.8420\n",
            "Epoch 135/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5421 - accuracy: 0.8751 - val_loss: 0.6195 - val_accuracy: 0.8426\n",
            "Epoch 136/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5412 - accuracy: 0.8753 - val_loss: 0.6188 - val_accuracy: 0.8426\n",
            "Epoch 137/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5403 - accuracy: 0.8756 - val_loss: 0.6181 - val_accuracy: 0.8427\n",
            "Epoch 138/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5394 - accuracy: 0.8758 - val_loss: 0.6171 - val_accuracy: 0.8428\n",
            "Epoch 139/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5385 - accuracy: 0.8758 - val_loss: 0.6165 - val_accuracy: 0.8432\n",
            "Epoch 140/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5375 - accuracy: 0.8763 - val_loss: 0.6158 - val_accuracy: 0.8429\n",
            "Epoch 141/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5366 - accuracy: 0.8766 - val_loss: 0.6150 - val_accuracy: 0.8431\n",
            "Epoch 142/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5357 - accuracy: 0.8768 - val_loss: 0.6142 - val_accuracy: 0.8430\n",
            "Epoch 143/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5347 - accuracy: 0.8770 - val_loss: 0.6136 - val_accuracy: 0.8432\n",
            "Epoch 144/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5338 - accuracy: 0.8771 - val_loss: 0.6127 - val_accuracy: 0.8434\n",
            "Epoch 145/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5329 - accuracy: 0.8776 - val_loss: 0.6121 - val_accuracy: 0.8438\n",
            "Epoch 146/300\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.5320 - accuracy: 0.8780 - val_loss: 0.6112 - val_accuracy: 0.8443\n",
            "Epoch 147/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5310 - accuracy: 0.8779 - val_loss: 0.6106 - val_accuracy: 0.8442\n",
            "Epoch 148/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5301 - accuracy: 0.8781 - val_loss: 0.6099 - val_accuracy: 0.8443\n",
            "Epoch 149/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5293 - accuracy: 0.8785 - val_loss: 0.6093 - val_accuracy: 0.8444\n",
            "Epoch 150/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5284 - accuracy: 0.8786 - val_loss: 0.6085 - val_accuracy: 0.8443\n",
            "Epoch 151/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5274 - accuracy: 0.8789 - val_loss: 0.6076 - val_accuracy: 0.8444\n",
            "Epoch 152/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5265 - accuracy: 0.8790 - val_loss: 0.6070 - val_accuracy: 0.8447\n",
            "Epoch 153/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5257 - accuracy: 0.8796 - val_loss: 0.6063 - val_accuracy: 0.8450\n",
            "Epoch 154/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5248 - accuracy: 0.8797 - val_loss: 0.6055 - val_accuracy: 0.8450\n",
            "Epoch 155/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5239 - accuracy: 0.8801 - val_loss: 0.6048 - val_accuracy: 0.8453\n",
            "Epoch 156/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5230 - accuracy: 0.8805 - val_loss: 0.6041 - val_accuracy: 0.8454\n",
            "Epoch 157/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5221 - accuracy: 0.8804 - val_loss: 0.6034 - val_accuracy: 0.8452\n",
            "Epoch 158/300\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.5212 - accuracy: 0.8809 - val_loss: 0.6027 - val_accuracy: 0.8457\n",
            "Epoch 159/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5204 - accuracy: 0.8810 - val_loss: 0.6020 - val_accuracy: 0.8462\n",
            "Epoch 160/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5195 - accuracy: 0.8813 - val_loss: 0.6014 - val_accuracy: 0.8460\n",
            "Epoch 161/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5187 - accuracy: 0.8816 - val_loss: 0.6007 - val_accuracy: 0.8458\n",
            "Epoch 162/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5178 - accuracy: 0.8816 - val_loss: 0.5999 - val_accuracy: 0.8463\n",
            "Epoch 163/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5169 - accuracy: 0.8820 - val_loss: 0.5993 - val_accuracy: 0.8468\n",
            "Epoch 164/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5161 - accuracy: 0.8820 - val_loss: 0.5986 - val_accuracy: 0.8466\n",
            "Epoch 165/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5152 - accuracy: 0.8823 - val_loss: 0.5979 - val_accuracy: 0.8471\n",
            "Epoch 166/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5144 - accuracy: 0.8826 - val_loss: 0.5972 - val_accuracy: 0.8469\n",
            "Epoch 167/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.5135 - accuracy: 0.8829 - val_loss: 0.5967 - val_accuracy: 0.8467\n",
            "Epoch 168/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5127 - accuracy: 0.8830 - val_loss: 0.5960 - val_accuracy: 0.8471\n",
            "Epoch 169/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5118 - accuracy: 0.8833 - val_loss: 0.5952 - val_accuracy: 0.8476\n",
            "Epoch 170/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.5110 - accuracy: 0.8832 - val_loss: 0.5946 - val_accuracy: 0.8474\n",
            "Epoch 171/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5102 - accuracy: 0.8837 - val_loss: 0.5939 - val_accuracy: 0.8481\n",
            "Epoch 172/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5093 - accuracy: 0.8838 - val_loss: 0.5932 - val_accuracy: 0.8476\n",
            "Epoch 173/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5085 - accuracy: 0.8841 - val_loss: 0.5924 - val_accuracy: 0.8480\n",
            "Epoch 174/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5077 - accuracy: 0.8844 - val_loss: 0.5919 - val_accuracy: 0.8480\n",
            "Epoch 175/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5069 - accuracy: 0.8845 - val_loss: 0.5911 - val_accuracy: 0.8481\n",
            "Epoch 176/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5061 - accuracy: 0.8848 - val_loss: 0.5906 - val_accuracy: 0.8483\n",
            "Epoch 177/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.5053 - accuracy: 0.8848 - val_loss: 0.5898 - val_accuracy: 0.8487\n",
            "Epoch 178/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5044 - accuracy: 0.8850 - val_loss: 0.5892 - val_accuracy: 0.8488\n",
            "Epoch 179/300\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.5036 - accuracy: 0.8852 - val_loss: 0.5887 - val_accuracy: 0.8486\n",
            "Epoch 180/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5028 - accuracy: 0.8854 - val_loss: 0.5878 - val_accuracy: 0.8484\n",
            "Epoch 181/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5020 - accuracy: 0.8854 - val_loss: 0.5873 - val_accuracy: 0.8493\n",
            "Epoch 182/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5012 - accuracy: 0.8854 - val_loss: 0.5866 - val_accuracy: 0.8492\n",
            "Epoch 183/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.5004 - accuracy: 0.8859 - val_loss: 0.5860 - val_accuracy: 0.8492\n",
            "Epoch 184/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4996 - accuracy: 0.8860 - val_loss: 0.5854 - val_accuracy: 0.8493\n",
            "Epoch 185/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4988 - accuracy: 0.8862 - val_loss: 0.5848 - val_accuracy: 0.8497\n",
            "Epoch 186/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4980 - accuracy: 0.8861 - val_loss: 0.5841 - val_accuracy: 0.8499\n",
            "Epoch 187/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4972 - accuracy: 0.8865 - val_loss: 0.5836 - val_accuracy: 0.8500\n",
            "Epoch 188/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4965 - accuracy: 0.8866 - val_loss: 0.5829 - val_accuracy: 0.8496\n",
            "Epoch 189/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4956 - accuracy: 0.8873 - val_loss: 0.5823 - val_accuracy: 0.8506\n",
            "Epoch 190/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4948 - accuracy: 0.8873 - val_loss: 0.5815 - val_accuracy: 0.8507\n",
            "Epoch 191/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4941 - accuracy: 0.8875 - val_loss: 0.5812 - val_accuracy: 0.8508\n",
            "Epoch 192/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4933 - accuracy: 0.8877 - val_loss: 0.5805 - val_accuracy: 0.8505\n",
            "Epoch 193/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4925 - accuracy: 0.8877 - val_loss: 0.5798 - val_accuracy: 0.8507\n",
            "Epoch 194/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4917 - accuracy: 0.8880 - val_loss: 0.5791 - val_accuracy: 0.8517\n",
            "Epoch 195/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4910 - accuracy: 0.8884 - val_loss: 0.5788 - val_accuracy: 0.8511\n",
            "Epoch 196/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4902 - accuracy: 0.8885 - val_loss: 0.5780 - val_accuracy: 0.8514\n",
            "Epoch 197/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4894 - accuracy: 0.8885 - val_loss: 0.5776 - val_accuracy: 0.8516\n",
            "Epoch 198/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4887 - accuracy: 0.8886 - val_loss: 0.5768 - val_accuracy: 0.8516\n",
            "Epoch 199/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4879 - accuracy: 0.8889 - val_loss: 0.5762 - val_accuracy: 0.8518\n",
            "Epoch 200/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4871 - accuracy: 0.8892 - val_loss: 0.5757 - val_accuracy: 0.8515\n",
            "Epoch 201/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4864 - accuracy: 0.8897 - val_loss: 0.5752 - val_accuracy: 0.8517\n",
            "Epoch 202/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4856 - accuracy: 0.8900 - val_loss: 0.5745 - val_accuracy: 0.8520\n",
            "Epoch 203/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4849 - accuracy: 0.8900 - val_loss: 0.5736 - val_accuracy: 0.8525\n",
            "Epoch 204/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4841 - accuracy: 0.8904 - val_loss: 0.5732 - val_accuracy: 0.8525\n",
            "Epoch 205/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4834 - accuracy: 0.8905 - val_loss: 0.5726 - val_accuracy: 0.8524\n",
            "Epoch 206/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4826 - accuracy: 0.8909 - val_loss: 0.5721 - val_accuracy: 0.8526\n",
            "Epoch 207/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4819 - accuracy: 0.8907 - val_loss: 0.5715 - val_accuracy: 0.8526\n",
            "Epoch 208/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4811 - accuracy: 0.8911 - val_loss: 0.5709 - val_accuracy: 0.8522\n",
            "Epoch 209/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4804 - accuracy: 0.8914 - val_loss: 0.5703 - val_accuracy: 0.8528\n",
            "Epoch 210/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4796 - accuracy: 0.8916 - val_loss: 0.5697 - val_accuracy: 0.8527\n",
            "Epoch 211/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4789 - accuracy: 0.8918 - val_loss: 0.5692 - val_accuracy: 0.8531\n",
            "Epoch 212/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4781 - accuracy: 0.8918 - val_loss: 0.5684 - val_accuracy: 0.8530\n",
            "Epoch 213/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4774 - accuracy: 0.8920 - val_loss: 0.5680 - val_accuracy: 0.8532\n",
            "Epoch 214/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4767 - accuracy: 0.8920 - val_loss: 0.5674 - val_accuracy: 0.8534\n",
            "Epoch 215/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4760 - accuracy: 0.8924 - val_loss: 0.5667 - val_accuracy: 0.8533\n",
            "Epoch 216/300\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 0.4752 - accuracy: 0.8923 - val_loss: 0.5662 - val_accuracy: 0.8542\n",
            "Epoch 217/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4745 - accuracy: 0.8926 - val_loss: 0.5656 - val_accuracy: 0.8537\n",
            "Epoch 218/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4738 - accuracy: 0.8930 - val_loss: 0.5651 - val_accuracy: 0.8542\n",
            "Epoch 219/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4731 - accuracy: 0.8928 - val_loss: 0.5646 - val_accuracy: 0.8540\n",
            "Epoch 220/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4724 - accuracy: 0.8931 - val_loss: 0.5639 - val_accuracy: 0.8540\n",
            "Epoch 221/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4717 - accuracy: 0.8931 - val_loss: 0.5634 - val_accuracy: 0.8538\n",
            "Epoch 222/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4709 - accuracy: 0.8935 - val_loss: 0.5628 - val_accuracy: 0.8543\n",
            "Epoch 223/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4702 - accuracy: 0.8935 - val_loss: 0.5622 - val_accuracy: 0.8544\n",
            "Epoch 224/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4695 - accuracy: 0.8937 - val_loss: 0.5618 - val_accuracy: 0.8543\n",
            "Epoch 225/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4688 - accuracy: 0.8939 - val_loss: 0.5611 - val_accuracy: 0.8548\n",
            "Epoch 226/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4681 - accuracy: 0.8939 - val_loss: 0.5605 - val_accuracy: 0.8547\n",
            "Epoch 227/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4673 - accuracy: 0.8942 - val_loss: 0.5602 - val_accuracy: 0.8548\n",
            "Epoch 228/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4667 - accuracy: 0.8943 - val_loss: 0.5595 - val_accuracy: 0.8554\n",
            "Epoch 229/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4660 - accuracy: 0.8941 - val_loss: 0.5589 - val_accuracy: 0.8548\n",
            "Epoch 230/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4653 - accuracy: 0.8945 - val_loss: 0.5584 - val_accuracy: 0.8553\n",
            "Epoch 231/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4646 - accuracy: 0.8946 - val_loss: 0.5579 - val_accuracy: 0.8556\n",
            "Epoch 232/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4639 - accuracy: 0.8949 - val_loss: 0.5572 - val_accuracy: 0.8556\n",
            "Epoch 233/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4632 - accuracy: 0.8948 - val_loss: 0.5568 - val_accuracy: 0.8555\n",
            "Epoch 234/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4625 - accuracy: 0.8950 - val_loss: 0.5563 - val_accuracy: 0.8561\n",
            "Epoch 235/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4618 - accuracy: 0.8951 - val_loss: 0.5559 - val_accuracy: 0.8561\n",
            "Epoch 236/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4612 - accuracy: 0.8952 - val_loss: 0.5552 - val_accuracy: 0.8558\n",
            "Epoch 237/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4605 - accuracy: 0.8957 - val_loss: 0.5548 - val_accuracy: 0.8561\n",
            "Epoch 238/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4598 - accuracy: 0.8956 - val_loss: 0.5543 - val_accuracy: 0.8562\n",
            "Epoch 239/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4591 - accuracy: 0.8957 - val_loss: 0.5536 - val_accuracy: 0.8569\n",
            "Epoch 240/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4584 - accuracy: 0.8957 - val_loss: 0.5531 - val_accuracy: 0.8569\n",
            "Epoch 241/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4578 - accuracy: 0.8960 - val_loss: 0.5525 - val_accuracy: 0.8571\n",
            "Epoch 242/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4571 - accuracy: 0.8960 - val_loss: 0.5520 - val_accuracy: 0.8566\n",
            "Epoch 243/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4563 - accuracy: 0.8961 - val_loss: 0.5517 - val_accuracy: 0.8572\n",
            "Epoch 244/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4557 - accuracy: 0.8964 - val_loss: 0.5512 - val_accuracy: 0.8574\n",
            "Epoch 245/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4550 - accuracy: 0.8965 - val_loss: 0.5504 - val_accuracy: 0.8574\n",
            "Epoch 246/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4544 - accuracy: 0.8967 - val_loss: 0.5499 - val_accuracy: 0.8582\n",
            "Epoch 247/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4537 - accuracy: 0.8969 - val_loss: 0.5494 - val_accuracy: 0.8581\n",
            "Epoch 248/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4530 - accuracy: 0.8970 - val_loss: 0.5488 - val_accuracy: 0.8581\n",
            "Epoch 249/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4523 - accuracy: 0.8974 - val_loss: 0.5484 - val_accuracy: 0.8575\n",
            "Epoch 250/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4517 - accuracy: 0.8973 - val_loss: 0.5479 - val_accuracy: 0.8581\n",
            "Epoch 251/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4510 - accuracy: 0.8975 - val_loss: 0.5472 - val_accuracy: 0.8581\n",
            "Epoch 252/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4504 - accuracy: 0.8975 - val_loss: 0.5468 - val_accuracy: 0.8581\n",
            "Epoch 253/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4497 - accuracy: 0.8976 - val_loss: 0.5461 - val_accuracy: 0.8584\n",
            "Epoch 254/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4490 - accuracy: 0.8978 - val_loss: 0.5458 - val_accuracy: 0.8583\n",
            "Epoch 255/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4484 - accuracy: 0.8981 - val_loss: 0.5452 - val_accuracy: 0.8582\n",
            "Epoch 256/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4478 - accuracy: 0.8982 - val_loss: 0.5446 - val_accuracy: 0.8587\n",
            "Epoch 257/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4471 - accuracy: 0.8984 - val_loss: 0.5441 - val_accuracy: 0.8589\n",
            "Epoch 258/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4464 - accuracy: 0.8986 - val_loss: 0.5438 - val_accuracy: 0.8589\n",
            "Epoch 259/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4458 - accuracy: 0.8986 - val_loss: 0.5432 - val_accuracy: 0.8593\n",
            "Epoch 260/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4451 - accuracy: 0.8988 - val_loss: 0.5426 - val_accuracy: 0.8593\n",
            "Epoch 261/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4444 - accuracy: 0.8990 - val_loss: 0.5422 - val_accuracy: 0.8596\n",
            "Epoch 262/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4438 - accuracy: 0.8992 - val_loss: 0.5418 - val_accuracy: 0.8593\n",
            "Epoch 263/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4432 - accuracy: 0.8992 - val_loss: 0.5412 - val_accuracy: 0.8593\n",
            "Epoch 264/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4425 - accuracy: 0.8993 - val_loss: 0.5405 - val_accuracy: 0.8593\n",
            "Epoch 265/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4419 - accuracy: 0.8997 - val_loss: 0.5402 - val_accuracy: 0.8594\n",
            "Epoch 266/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4413 - accuracy: 0.8996 - val_loss: 0.5398 - val_accuracy: 0.8596\n",
            "Epoch 267/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4406 - accuracy: 0.8997 - val_loss: 0.5393 - val_accuracy: 0.8599\n",
            "Epoch 268/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4400 - accuracy: 0.9000 - val_loss: 0.5387 - val_accuracy: 0.8598\n",
            "Epoch 269/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4393 - accuracy: 0.9002 - val_loss: 0.5382 - val_accuracy: 0.8601\n",
            "Epoch 270/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4387 - accuracy: 0.9001 - val_loss: 0.5377 - val_accuracy: 0.8602\n",
            "Epoch 271/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4380 - accuracy: 0.9005 - val_loss: 0.5373 - val_accuracy: 0.8605\n",
            "Epoch 272/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4374 - accuracy: 0.9003 - val_loss: 0.5368 - val_accuracy: 0.8608\n",
            "Epoch 273/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4368 - accuracy: 0.9006 - val_loss: 0.5364 - val_accuracy: 0.8607\n",
            "Epoch 274/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4362 - accuracy: 0.9006 - val_loss: 0.5358 - val_accuracy: 0.8608\n",
            "Epoch 275/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4355 - accuracy: 0.9012 - val_loss: 0.5353 - val_accuracy: 0.8607\n",
            "Epoch 276/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4349 - accuracy: 0.9013 - val_loss: 0.5348 - val_accuracy: 0.8610\n",
            "Epoch 277/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4342 - accuracy: 0.9013 - val_loss: 0.5343 - val_accuracy: 0.8609\n",
            "Epoch 278/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4336 - accuracy: 0.9015 - val_loss: 0.5341 - val_accuracy: 0.8612\n",
            "Epoch 279/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4330 - accuracy: 0.9016 - val_loss: 0.5335 - val_accuracy: 0.8611\n",
            "Epoch 280/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4324 - accuracy: 0.9017 - val_loss: 0.5330 - val_accuracy: 0.8614\n",
            "Epoch 281/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4318 - accuracy: 0.9016 - val_loss: 0.5325 - val_accuracy: 0.8614\n",
            "Epoch 282/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4311 - accuracy: 0.9020 - val_loss: 0.5321 - val_accuracy: 0.8615\n",
            "Epoch 283/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4305 - accuracy: 0.9022 - val_loss: 0.5316 - val_accuracy: 0.8616\n",
            "Epoch 284/300\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.4299 - accuracy: 0.9020 - val_loss: 0.5311 - val_accuracy: 0.8616\n",
            "Epoch 285/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4293 - accuracy: 0.9024 - val_loss: 0.5305 - val_accuracy: 0.8623\n",
            "Epoch 286/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4287 - accuracy: 0.9025 - val_loss: 0.5301 - val_accuracy: 0.8618\n",
            "Epoch 287/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4281 - accuracy: 0.9027 - val_loss: 0.5297 - val_accuracy: 0.8619\n",
            "Epoch 288/300\n",
            "21/21 [==============================] - 1s 69ms/step - loss: 0.4274 - accuracy: 0.9026 - val_loss: 0.5293 - val_accuracy: 0.8621\n",
            "Epoch 289/300\n",
            "21/21 [==============================] - 1s 66ms/step - loss: 0.4268 - accuracy: 0.9030 - val_loss: 0.5288 - val_accuracy: 0.8623\n",
            "Epoch 290/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4262 - accuracy: 0.9031 - val_loss: 0.5283 - val_accuracy: 0.8623\n",
            "Epoch 291/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4256 - accuracy: 0.9032 - val_loss: 0.5278 - val_accuracy: 0.8629\n",
            "Epoch 292/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4250 - accuracy: 0.9034 - val_loss: 0.5274 - val_accuracy: 0.8625\n",
            "Epoch 293/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4244 - accuracy: 0.9034 - val_loss: 0.5268 - val_accuracy: 0.8626\n",
            "Epoch 294/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4238 - accuracy: 0.9038 - val_loss: 0.5263 - val_accuracy: 0.8633\n",
            "Epoch 295/300\n",
            "21/21 [==============================] - 1s 68ms/step - loss: 0.4232 - accuracy: 0.9038 - val_loss: 0.5261 - val_accuracy: 0.8625\n",
            "Epoch 296/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4226 - accuracy: 0.9043 - val_loss: 0.5256 - val_accuracy: 0.8634\n",
            "Epoch 297/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4220 - accuracy: 0.9041 - val_loss: 0.5251 - val_accuracy: 0.8638\n",
            "Epoch 298/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4214 - accuracy: 0.9043 - val_loss: 0.5246 - val_accuracy: 0.8636\n",
            "Epoch 299/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4208 - accuracy: 0.9044 - val_loss: 0.5241 - val_accuracy: 0.8636\n",
            "Epoch 300/300\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.4202 - accuracy: 0.9045 - val_loss: 0.5236 - val_accuracy: 0.8640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6rZyaVLcwqj",
        "outputId": "1ed897e7-62dd-42c9-9226-aaf1a07d88e7"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}